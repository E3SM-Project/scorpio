<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>SCORPIO: Pio_init</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">SCORPIO
   &#160;<span id="projectnumber">1.6.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions/Subroutines</a>  </div>
  <div class="headertitle">
<div class="title">Pio_init</div>  </div>
</div><!--header-->
<div class="contents">

<p>Initialize the I/O subsystem.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions/Subroutines</h2></td></tr>
<tr class="memitem:gaea83c0882a1c53bc92bafa558820d067"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__pio__init.html#gaea83c0882a1c53bc92bafa558820d067">spio_init::pio_init_intracomm</a> (comm_rank, comm, nioprocs, naggprocs, ioprocs_stride, rearr, iosys, base, rearr_opts, ierr)</td></tr>
<tr class="memdesc:gaea83c0882a1c53bc92bafa558820d067"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initialize the I/O subsystem that is defined using an MPI intra-communicator. This is a collective call on the MPI communicator, <code>comm</code>, used to initialize the I/O subsystem.  <a href="group__pio__init.html#gaea83c0882a1c53bc92bafa558820d067">More...</a><br /></td></tr>
<tr class="separator:gaea83c0882a1c53bc92bafa558820d067"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gadbcc295406465eb34a911de748060d6b"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__pio__init.html#gadbcc295406465eb34a911de748060d6b">spio_init::pio_init_intercomm</a> (ncomps, peer_comm, comp_comms, io_comm, iosys, rearr, ierr)</td></tr>
<tr class="memdesc:gadbcc295406465eb34a911de748060d6b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initialize the I/O subsystem that is defined using an MPI inter-communicator. The I/O subsystem is created using one or more groups of compute processes (Defined by an array of MPI communicators, <code>comp_comms</code>, where each MPI communicator in the array corresponds to a group of compute processes. The compute and I/O processes are disjoint set of processes. Note that typically each MPI communicator/process_group is associated with an computational component in the application) and a group of I/O processes (Defined by an MPI communicator, <code>io_comm</code>). The compute processes redirect all I/O requests (reading/writing variables, attributes etc) to the I/O processes by asynchronously communicating with the I/O processes (via asynchronous messages)  <a href="group__pio__init.html#gadbcc295406465eb34a911de748060d6b">More...</a><br /></td></tr>
<tr class="separator:gadbcc295406465eb34a911de748060d6b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga9c69552695fc2b00940b5a6dba74a095"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__pio__init.html#ga9c69552695fc2b00940b5a6dba74a095">spio_init::pio_init_intercomm_v2</a> (iosys, peer_comm, comp_comms, io_comm, rearr, ierr)</td></tr>
<tr class="memdesc:ga9c69552695fc2b00940b5a6dba74a095"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initialize the I/O subsystem that is defined using an MPI inter-communicator. The I/O subsystem is created using one or more groups of compute processes (Defined by an array of MPI communicators, <code>comp_comms</code>, where each MPI communicator in the array corresponds to a group of compute processes. The compute and I/O processes are disjoint set of processes. Note that typically each MPI communicator/process_group is associated with an computational component in the application) and a group of I/O processes (Defined by an MPI communicator, <code>io_comm</code>). The compute processes redirect all I/O requests (reading/writing variables, attributes etc) to the I/O processes by asynchronously communicating with the I/O processes (via asynchronous messages)  <a href="group__pio__init.html#ga9c69552695fc2b00940b5a6dba74a095">More...</a><br /></td></tr>
<tr class="separator:ga9c69552695fc2b00940b5a6dba74a095"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>Initialize the I/O subsystem. </p>
<p>Each I/O subsystem (corresponding to a set of MPI processes that belong to an MPI communicator and collectively work on a set of files) needs to be initialized before invoking any PIO APIs on the subsystem </p>
<h2 class="groupheader">Function/Subroutine Documentation</h2>
<a id="gadbcc295406465eb34a911de748060d6b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gadbcc295406465eb34a911de748060d6b">&#9670;&nbsp;</a></span>pio_init_intercomm()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine spio_init::pio_init_intercomm </td>
          <td>(</td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>ncomps</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>peer_comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, dimension(ncomps), intent(in), target&#160;</td>
          <td class="paramname"><em>comp_comms</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>io_comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">type(<a class="el" href="structpio__types_1_1iosystem__desc__t.html">iosystem_desc_t</a>), dimension(:), intent(out)&#160;</td>
          <td class="paramname"><em>iosys</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in), optional&#160;</td>
          <td class="paramname"><em>rearr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(out), optional&#160;</td>
          <td class="paramname"><em>ierr</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Initialize the I/O subsystem that is defined using an MPI inter-communicator. The I/O subsystem is created using one or more groups of compute processes (Defined by an array of MPI communicators, <code>comp_comms</code>, where each MPI communicator in the array corresponds to a group of compute processes. The compute and I/O processes are disjoint set of processes. Note that typically each MPI communicator/process_group is associated with an computational component in the application) and a group of I/O processes (Defined by an MPI communicator, <code>io_comm</code>). The compute processes redirect all I/O requests (reading/writing variables, attributes etc) to the I/O processes by asynchronously communicating with the I/O processes (via asynchronous messages) </p>
<p>This is a collective call on the MPI communicator, <code>peer_comm</code>, used to initialize the I/O subsystem. For all compute processes this API returns after the I/O subsystem is initialized. The API blocks inside the library for all I/O processes waiting for asynchronous messages from the compute processes and returns once the I/O subsystems are finalized (via PIO_finalize()) on all compute processes.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ncomps</td><td>The number of compute components (Also determines the size of the <code>comp_comms</code> array) </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">peer_comm</td><td>The peer MPI communicator to use to create an MPI inter-communicator between compute and I/O MPI communicators. This is typically the MPI communicator from which the component and I/O communicators are created (MPI_COMM_WORLD or the "world communicator" for all the MPI communicators) </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">comp_comms</td><td>An array of MPI communicators corresponding to the groups of compute processes. Typically each application component has an MPI communicator, containing all the processes in the component, in this array. This array needs to contain at least <code>ncomps</code> elements, and all processes belonging to compute component i (a unique global index decided by the application for each component) provides the MPI communicator for component i in comp_comms(i). All I/O processes pass MPI_COMM_NULL for all communicators in this array #param[in] io_comm The MPI communicator containing all the I/O processes. The I/O processes are separate (disjoint set) from the compute processes All I/O processes provide the MPI communicator containing all the I/O processes via this argument. All compute processes pass MPI_COMM_NULL in this argument. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">iosys</td><td>The handle to the initialized I/O system is returned via this array. For compute component i (a unique global index decided by the application for each component) the initialized I/O subsystem is returned in the ith index of this array, iosys(i). IO system descriptor structure. This structure contains the general IO subsystem data and MPI structure </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rearr</td><td>(Optional) The three choices to control rearrangement are:<ul>
<li>PIO_rearr_none : Do not use any form of rearrangement</li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>
<ul>
<li>PIO_rearr_box : Use a PIO internal box rearrangement</li>
</ul>
<p>PIO_rearr_subset : Use a PIO internal subsetting rearrangement </p><dl class="retval"><dt>Return values</dt><dd>
  <table class="retval">
    <tr><td class="paramname">ierr</td><td>(Optional) : The error return code. Set to PIO_NOERR on success, or an error code otherwise (See <a class="el" href="group___p_i_o__seterrorhandling.html">PIO_seterrorhandling</a> for more information on how to customize/set error handling)</td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ga9c69552695fc2b00940b5a6dba74a095"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga9c69552695fc2b00940b5a6dba74a095">&#9670;&nbsp;</a></span>pio_init_intercomm_v2()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine spio_init::pio_init_intercomm_v2 </td>
          <td>(</td>
          <td class="paramtype">type(<a class="el" href="structpio__types_1_1iosystem__desc__t.html">iosystem_desc_t</a>), dimension(:), intent(out)&#160;</td>
          <td class="paramname"><em>iosys</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>peer_comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, dimension(:), intent(in), target&#160;</td>
          <td class="paramname"><em>comp_comms</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>io_comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in), optional&#160;</td>
          <td class="paramname"><em>rearr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(out), optional&#160;</td>
          <td class="paramname"><em>ierr</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Initialize the I/O subsystem that is defined using an MPI inter-communicator. The I/O subsystem is created using one or more groups of compute processes (Defined by an array of MPI communicators, <code>comp_comms</code>, where each MPI communicator in the array corresponds to a group of compute processes. The compute and I/O processes are disjoint set of processes. Note that typically each MPI communicator/process_group is associated with an computational component in the application) and a group of I/O processes (Defined by an MPI communicator, <code>io_comm</code>). The compute processes redirect all I/O requests (reading/writing variables, attributes etc) to the I/O processes by asynchronously communicating with the I/O processes (via asynchronous messages) </p>
<p>This is a collective call on the MPI communicator, <code>peer_comm</code>, used to initialize the I/O subsystem. For all compute processes this API returns after the I/O subsystem is initialized. The API blocks inside the library for all I/O processes waiting for asynchronous messages from the compute processes and returns once the I/O subsystems are finalized (via PIO_finalize()) on all compute processes.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[out]</td><td class="paramname">iosys</td><td>The handle to the initialized I/O system is returned via this array. For compute component i (a unique global index decided by the application for each component) the initialized I/O subsystem is returned in the ith index of this array, iosys(i). IO system descriptor structure. This structure contains the general IO subsystem data and MPI structure </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">peer_comm</td><td>The peer MPI communicator to use to create an MPI inter-communicator between compute and I/O MPI communicators. This is typically the MPI communicator from which the component and I/O communicators are created (MPI_COMM_WORLD or the "world communicator" for all the MPI communicators) </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">comp_comms</td><td>An array of MPI communicators corresponding to the groups of compute processes. Typically each application component has an MPI communicator, containing all the processes in the component, in this array. All processes belonging to compute component i (a unique global index decided by the application for each component) provides the MPI communicator for component i in comp_comms(i). All I/O processes pass MPI_COMM_NULL for all communicators in this array #param[in] io_comm The MPI communicator containing all the I/O processes. The I/O processes are separate (disjoint set) from the compute processes All I/O processes provide the MPI communicator containing all the I/O processes via this argument. All compute processes pass MPI_COMM_NULL in this argument. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rearr</td><td>(Optional) The three choices to control rearrangement are:<ul>
<li>PIO_rearr_none : Do not use any form of rearrangement</li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>
<ul>
<li>PIO_rearr_box : Use a PIO internal box rearrangement</li>
</ul>
<p>PIO_rearr_subset : Use a PIO internal subsetting rearrangement </p><dl class="retval"><dt>Return values</dt><dd>
  <table class="retval">
    <tr><td class="paramname">ierr</td><td>(Optional) : The error return code. Set to PIO_NOERR on success, or an error code otherwise (See <a class="el" href="group___p_i_o__seterrorhandling.html">PIO_seterrorhandling</a> for more information on how to customize/set error handling)</td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="gaea83c0882a1c53bc92bafa558820d067"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaea83c0882a1c53bc92bafa558820d067">&#9670;&nbsp;</a></span>pio_init_intracomm()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine spio_init::pio_init_intracomm </td>
          <td>(</td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>comm_rank</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>nioprocs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>naggprocs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>ioprocs_stride</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>rearr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">type(<a class="el" href="structpio__types_1_1iosystem__desc__t.html">iosystem_desc_t</a>), intent(out)&#160;</td>
          <td class="paramname"><em>iosys</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in), optional&#160;</td>
          <td class="paramname"><em>base</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">type(<a class="el" href="structpio__types_1_1pio__rearr__opt__t.html">pio_rearr_opt_t</a>), intent(in), optional, target&#160;</td>
          <td class="paramname"><em>rearr_opts</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(out), optional&#160;</td>
          <td class="paramname"><em>ierr</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Initialize the I/O subsystem that is defined using an MPI intra-communicator. This is a collective call on the MPI communicator, <code>comm</code>, used to initialize the I/O subsystem. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">comm_rank</td><td>The rank (in <code>comm</code>) of the MPI process initializing the I/O subsystem </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">comm</td><td>The MPI communicator containing all the MPI processes initializing the I/O subsystem </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">nioprocs</td><td>The total number of I/O processes (a subset of the total number of processes in <code>comm</code>) in the I/O subsystem </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">naggprocs</td><td>The total number of processes aggregating data in the I/O subsystem (this argument is ignored now, and is retained for API backward compatibility) </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">ioprocs_stride</td><td>The stride (The number of MPI processes to "skip" between assigning two consecutive I/O processes. A stride of 1 implies consecutive MPI processes after the <code>ioprocs_base</code> rank are designated the I/O processes) between two I/O processes in the I/O subsystem </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rearr</td><td>The three choices to control rearrangement are:<ul>
<li>PIO_rearr_none : Do not use any form of rearrangement</li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>
<ul>
<li>PIO_rearr_box : Use a PIO internal box rearrangement</li>
</ul>
<p>PIO_rearr_subset : Use a PIO internal subsetting rearrangement </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[out]</td><td class="paramname">iosys</td><td>The handle to the initialized I/O system is returned via this argument. IO system descriptor structure. This structure contains the general IO subsystem data and MPI structure </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">base</td><td>(Optional) The base (MPI rank of the 1st I/O process) rank for I/O processes. By default, MPI rank 0 (in <code>comm</code>) is the base I/O process </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rearr_opts</td><td>(Optional) The I/O rearranger options to use for this I/O subsystem. The data rearranger options. The library includes support for a data rearranger that rearranges data among MPI processes to improve the I/O throughput of the application. The user can control the data rearrangement by passing the data rearranger options to the library.<ul>
<li>comm_type : The data rearranger communication mode. The data rearranger in the library rearranges data between the compute processes (all the MPI processes in the I/O subsystem) and I/O processes (a subset of the compute processes in the I/O subsystem or a disjoint set of MPI processes in the I/O subsystem) before flushing the data written out by the user to the filesystem. The data rearrangement among the MPI processes can be achieved using MPI point to point or collective communication.</li>
<li>PIO_rearr_comm_p2p : Point to point</li>
<li>PIO_rearr_comm_coll : Collective</li>
<li>fcd : The data rearrangment flow control direction. The data rearranger can use flow control when rearranging data among the MPI processes. The flow control can be enabled for data rearrangement from compute processes to I/O processes (e.g. during a write operation), from I/O processes to compute processes (e.g. during a read operation) or both.</li>
<li>PIO_rearr_comm_fc_2d_enable : compute procs to I/O procs and vice versa</li>
<li>PIO_rearr_comm_fc_1d_comp2io: compute procs to I/O procs only</li>
<li>PIO_rearr_comm_fc_1d_io2comp: I/O procs to compute procs only</li>
<li>PIO_rearr_comm_fc_2d_disable: disable flow control</li>
<li>comm_fc_opts : The data rearranger flow control options. The data rearranger supports flow control when rearranging data among the MPI processes.</li>
<li>enable_hs : Enable handshake (logical). If this option is enabled a "handshake" is sent between communicating processes before data is sent</li>
<li>enable_isend : Enable non-blocking sends (logical). If this option is enabled non-blocking sends (instead of blocking sends) are used to send data between MPI processes while rearranging data</li>
<li>max_pend_req : The maximum pending requests allowed at a time when MPI processes communicate for rearranging data among themselves. (Use PIO_REARR_COMM_UNLIMITED_PEND_REQ for allowing unlimited number of pending requests) </li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>
<dl class="retval"><dt>Return values</dt><dd>
  <table class="retval">
    <tr><td class="paramname">ierr</td><td>(Optional) : The error return code. Set to PIO_NOERR on success, or an error code otherwise (See <a class="el" href="group___p_i_o__seterrorhandling.html">PIO_seterrorhandling</a> for more information on how to customize/set error handling)</td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sun Mar 10 2024 20:18:09 for SCORPIO by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
